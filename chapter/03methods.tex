%!TEX root = ../thesis.tex
\chapter{Methods}
\label{ch:03methods}

% Chapter Introduction:
% Outlines the scope and structure of the Methods chapter.

In this chapter, we introduce the methods used in this thesis.
Our research hypothesis states that the navigation capabilities of large language models may be substantially improved by the inclusion of qualitative geographic context.
To test our hypothesis, we designed an experimental setup involving large language models, qualitative geographic context and navigation tasks.
Our approach can be broken down into the data used, the configuration of the LLMs - including the context enrichment setup -, and ultimately the procedures used to conduct the experiments.
To evaluate the results, we define a strict correctness criterion and determine success rates for the control and test groups.


% Data Acquisition and Preparation:
% Describes how the data used in the experiments was collected and prepared.

\section{Data Acquisition and Preparation}

% Why the data is not readily available on the internet.

The first step in our experimental setup is the generation of dipole relations describing topological relationships in street networks which can then be used as qualitative geographic context for our LLMs.
While high quality geographic datasets on street networks are available at no additional cost from various sources such as OpenStreetMap, these datasets typically do not include qualitative geographic descriptions.
The most common way of encountering street network data is in the form of GraphML files which describe the street network as a graph of nodes and edges.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{chapter/graphics/hamburg_network.png} % Reduced to 60% of text width
    \caption{Area of interest in Hamburg. Data \copyright{} OpenStreetMap contributors.}
    \label{fig:hamburg-graph}
\end{figure}

% Which data was downloaded and how was it processed to generate the dipole relations.

As a basis for our experiments, we downloaded two of these graphs from the OpenStreetMap database using OSMnx, a powerful Python library capable of handling and processing OpenStreetMap data.
In order to ensure a reasonable degree of geographic variation, we selected two different cities as sites for our datasets: Hamburg and Münster.
This way, we end up with one dataset per city, each containing a varying number of nodes and edges.
To extract qualitative descriptions from these datasets, an algorithm capable of generating dipole relations in natural language describing the topology of a given graph was used.
A conceptual overview of this algorithm is provided in Algorithm~\ref{alg:dipole-generation}:

\begin{algorithm}[H]
\caption{Generate Street Descriptions from OSM.}
\label{alg:dipole-generation}
\begin{algorithmic}[1]
\Require Bounding box
\Ensure Text descriptions for streets in bounding box
\State Download road network for bounding box
\State Remove motorways and isolated nodes
\For{each street name in network}
    \State Collect all road segments with this name
    \State Keep largest connected component
    \State Orient path consistently
    \For{each junction along path}
        \State Inspect connected streets
        \State Use street orientations to classify as left/right/crossing
        \State Generate natural-language statement
    \EndFor
\EndFor
\State Export all descriptions to file
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:dipole-generation} describes the core logic of our dipole generation process.
After downloading the street network for a given bounding box, the algorithm first removes motorways and isolated nodes from the dataset.
Then, for each street in the network, all road segments with the same name are collected.
The largest connected component is kept to avoid disconnected segments.
This component is then oriented and for each junction along the path the connected streets are inspected.
Using the street orientations, the connected streets are classified as branching left, branching right or crossing.
Finally, a natural-language statement is generated for each junction.
The natural-language statements are then exported to a text file for later use.
In the above description of the algorithm, we have omitted various implementation details to ensure clarity.
The term ``require'' is used to denote the input to the algorithm (in our case a bounding box), while the term ``ensure'' denotes the output of the algorithm (in our case text descriptions for streets).
The Python software used to generate the qualitative dipole relations was primarily implemented by James Ondieki, and the code is available upon request.
In Figure~\ref{fig:hamburg-graph} we see the network-graph used for this study as extracted from OpenStreetMap via OSMnx after some preprocessing as described in Algorithm~\ref{alg:dipole-generation}.

% How was the data stored and how can the data be described.

Following this method, we were able to generate a set of dipole relations for each of our initial street network datasets.
These relations were stored in the form of simple .txt files, with one relation per line.
An example of these relations is given in the appendix. 
To demonstrate the relations here, we show an excerpt regarding the Street Halbmondsweg in Hamburg:

\begin{tcolorbox}[title={Excerpt of qualitative relations (Hamburg)}, colback=white]
\small
\begin{verbatim}
Halbmondsweg

Halbmondsweg begins at the intersection with Bernadottestraße.
Bernadottestraße then branches off to the left.
Ansorgestraße then branches off to the left.
Borchlingweg then branches off to the right.
Agathe-Lasch-Weg then branches off to the right.
Klein Flottbeker Weg then branches off to the left.
Reventlowstraße then branches off to the right.
\end{verbatim}
\end{tcolorbox}

After discussing the data acquisition process, we now focus on a detailed description of the configuration of the LLMs used in this study.

% Which LLMs were selected and how were they configured.

\section{LLM Configuration}

% Why using the proprietary web interfaces is impractical for this study.

The most common way for users to interact with large language models is to use their proprietary web interfaces.
However, we identified this approach to be impractical for our needs:
we aim to evaluate multiple configurations of LLMs in a systematic manner.
One solution would be to use the APIs provided by the LLM vendors to create a proprietary program to interact with the models.
However, this approach would have introduced additional programming overhead to our experimental setup.

% Why openrouter was selected and how the platform works.

One alternative solution to access a wide range of current or legacy LLMs through a unified interface is to use a platform like OpenRouter, which is the platform we have chosen to use for this study.
Through its web-interface it is possible to query multiple LLMs concurrently without any additional programming effort.
OpenRouter usage is not free, and usage costs depend on the selected models and the amount of tokens processed.
For this thesis, the total budget for OpenRouter usage did not exceed 20 EUR.
OpenRouter also enables prompting of multiple models a the same time.
This made it possible to conduct our experiments in a consistent manner and reduced the total time required to complete the trials.
However, this configuration does not mean that the models used shared a context window when being queried at the same time.
Rather, their answers are generated seperately from each other and then returned to OpenRouter for display to the user.
To demonstrate the usage of OpenRouter, we provide a depiction of the user interface in Figure~\ref{fig:openrouter-interface}:

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{chapter/graphics/openrouter.png} % Reduced to 60% of text width
    \caption{Demonstration of OpenRouter usage.}
    \label{fig:openrouter-interface}
\end{figure}

The prompt used in this example was:

\begin{quote}
    \textit{What is the capital of Iceland?}
\end{quote}

The models, in this case GPT-4o, Gemini 2.5 Pro and Claude Sonnet 4.5 (as used in the remainder of this study), each generated a response and returned it to OpenRouter for display.
In the Figure~\ref{fig:openrouter-interface}, we see that GPT-4o was first to respond, while Gemini 2.5 Pro and Claude Sonnet 4.5 were still processing the prompt.
The exact response contents of this example are not relevant for this thesis and are thus left our here.
Response times can vary between models, and in some cases the responses even took several minutes to be generated.

% Which models were selected and why.

New models arrive on the market frequently, making it difficult to test ``the latest'' or ``state of the art'' models.
After all, a model with significantly greater capabilities in various areas could be just around the corner at any given time.
Nevertheless, we have selected three models which we consider to be widely used.
Our selection of models for this study therefore consists of OpenAI's GPT-4o, Google's Gemini 2.5 Pro as well as Anthropic's Claude Sonnet 4.5.
Table~\ref{tab:selected-models} provides an overview of the models selected for this study.

\begin{table}[h]
\centering
\begin{tabular}{l l}
\hline
\textbf{Model} & \textbf{Provider} \\
\hline
GPT-4o & OpenAI \\
Gemini 2.5 Pro & Google \\
Claude Sonnet 4.5 & Anthropic \\
\hline
\end{tabular}
\caption{LLMs selected for this study.}
\label{tab:selected-models}
\end{table}


% How was the qualitative context provided to the models?

The symbolic dipole relations generated using the Algorithm~\ref{alg:dipole-generation} in the previous section were stored using text files.
For the navigation trials in the test group, these text files were provided to the models using OpenRouter's file upload (attachment) feature,.
This way, the models in the test group had access to the qualitative geographic context, emulating a graph-RAG type of setup.

In the control group, we wanted to ensure that the models used their inherent knowledge only, without any additional context or tools.
To achieve this, we used multiple strategies.
First, we made sure that no files were attached to the model queries in the control group.
Second, we switched off any web-search tools available to the models.
Lastly, we used a specific system prompt to instruct the models in the control group to rely on their own knowledge.

In the test group, we also used a specific system prompt.
This system prompt told the models to use the provided dipole relations when figuring out a route.
The exact system prompts we used in our experiments are provided in the appendix.
Apart from the system prompts and the file attachments, the models in both groups were configured identically:
all other parameteres such as temperature, max tokens and top-p were left at their default values in OpenRouter.
This should make the experiments easy to replicate for future researchers.

% How was the experimental procedure designed?

\section{Experimental Design and Procedure}

% Traditional A/B testing approach with Test and Control Group.

Our experimental design follows a traditional A/B testing approach, where we compare the performance of a Control Group against the performance of a Test Group.
In our case, the Control Group consists of LLMs performing navigation tasks on their own, without any additional context.
Respectively, the Test Group consists of LLMs performing the same navigation tasks, but with access to our dipole relations.
While the Test Group performs the exact same tasks as the Control Group, it was ensured that no memories of previous trials were available to the models in the Test Group sessions by starting each individual trial of both groups in a fresh session, disconnected from any previously conducted trials.

% How were the navigation tasks designed and executed?

To reduce the risk of bias towards certain areas or street types, we designed navigation tasks which consist of two street names (start street and end street), both derived randomly from the available street names.
The first step was to extract a list of unique street names from both datasets (Hamburg and Münster).
These lists were then sampled uniformly at random 20 times with replacement each to generate start streets for the navigation tasks.
To make sure that we receive 20 unique start streets per dataset, any duplicates were removed and replaced by new random samples.
This process was repeated to generate the end streets for the navigation tasks.
In case any start and end street pairs were identical, these pairs were also replaced by new random samples.
In a final check, we ensured that all generated navigation tasks were unique within each dataset.
In total, this results in 40 unique navigation tasks (20 per dataset).
A subset of example tasks from the Hamburg dataset is provided in the appendix.
In the following, we refer to a task as a single Start-to-End street navigation problem, while a trial refers to a single model response to a specific task.
Each navigation task was evaluated once per model and context condition.
This results in a total of 240 trials (3 models \( \times \) 2 context conditions \( \times \) 40 navigation tasks).

% How were the trial responses stored?

After trial execution, the model responses were stored using a spreadsheet.
In case of exceedingly verbose model responses, the text was truncated, and only the relevant instuctions for the navigation task were stored.

% How was the obtained data evaluated and which statistical tools were used?

\section{Evaluation Metrics}

% Responses were reviewed manually against Google Maps.

All responses were then reviewed manually by comparing the generated instructions saved in the spreadsheet against Google Maps.
When looking at any response, the objective was to determine whether the produced steps could be executed in sequence to solve the navigation task.
If this was the case, the response was labelled as correct, otherwise it was labelled as incorrect.
Since the responses were labelled manually by a single evaluator, they may contain some subjective judgement in a few borderline cases.

% What does correct/incorrect mean in this study?

This means that just one incorrect instruction in any LLM response, control or test group, would consequently render the entire response incorrect.
A trial was labelled as incorrect if the response contained at least one instruction that (i) referenced a non-existing street, (ii) required an impossible turn, or (iii) produced a disconnected route.
Our validation does not take into account any further measures like distance measures or number of turns.
While these additional measures could provide interesting insights, they are outside the scope of this study.
The success rate is therefore defined as the proportion of navigation trials which were labelled as correct over the total number of navigation trials conducted, as described in Equation~\ref{eq:success-rate}:

\begin{equation}
\label{eq:success-rate}
\text{Success Rate} \;=\; \frac{N_{\text{correct}}}{N_{\text{total}}}
\end{equation}


% How was the success rate calculated?

After labelling all model responses manually, the results were aggregated and success rates were calculated.
This concludes our description of the experimental methods used in this study.
In the following chapter, we will present the results obtained from our experiments.