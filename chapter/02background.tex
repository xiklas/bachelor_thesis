\chapter{Background}
\label{ch:02background}

% Chapter Introduction
% Scope & Structure

This chapter provides the technical and theoretical background on the topic of navigation using large language models as well as on related topics which are essential for this thesis.
We will first discuss the general capabilities of LLMs, before narrowing down on their performance in spatial reasoning. Afterwards, we will review research on qualitative representations of topological data as well as context enrichment techniques for LLMs and observe if these techniques have been applied to study LLM performance in navigation tasks.


% What is an LLM?
\section{Large Language Models}

The large language model, commonly abbreviated as "LLM", is the central technology this study revolves around.
While the history of LLMs is a fascinating topic in its own and can be traced back to first attempts at language modeling in the 1950s \citep{minaee_large_2025}, it is too extensive to be covered in this thesis, and also not what our research is about.
Therefore, we are going to focus on a purely practical overview on LLMs and their relevant characteristics to this study.

Large language models are the latest development and a specific subclass of language models in general \citep{minaee_large_2025}.
In practice, the challenge large language models aim to address is the prediction of the next token in a given sequence of tokens \citep{minaee_large_2025}.
For example, given the input tokens (tokens can be thought of as words or parts of words) "Roses are", a large language model would attempt to predict the next token, which in this case could be "red".
Adding the predicted token to the input sequence, the model could then attempt to predict the next token and so on.
This process is known as autoregressive generation and applicable in other domains such as computer vision as well \citep{xiong_autoregressive_2025}.
To continue with the previous example, the next tokens after "Roses are red" could be "violet", with the prediction process eventually resulting in the sentence "Roses are red, violets are blue".

Creating a model that is able to perform this kind of token prediction and deliver acceptable results is by no means a trivial endeavor.
In fact, one of the biggest challenges with earlier language models such as RNNs (recurrent neural networks) was that they were often unable to maintain cohesion over longer sequences of tokens, making them unsuitable for practical use cases \citep{johnston_revisiting_2025}.


Modern large language models mitigate this issue by making use of several key enabling factors, the first one we will discuss is the transformer architecture.
The transformer architecture allows every token in a sequence to attend every other token during token prediction, enabling coherence over much longer sequences.
This architecture itself is realised through self-attention layers in the model architecture \citep{vaswani_attention_2017}.
All experiments conducted in this thesis are therefore partially enabled by this breakthrough architecture, without which the generation of lengthy, coherent text sequences using the LLMs we employ today would probably not be possible. 

Another key factor driving large language models is the availability of sufficient training data.
To achieve high performance, LLMs need to be trained on vast amounts of text data.
This data is usually scraped from books or the public internet.
Although exact figures on training dataset sizes are difficult to obtain for current models, even early models were trained on datasets containing billions of tokens \citep{minaee_large_2025}.

In the pre-training process, the model is trained using the training data, often to minimize prediction error.
This means that the model, consisting of a substantial number of parameters (often billions) is adjusted iteratively.
In each iteration, the model makes predictions and the prediction error is calculated.
Using this error, the parameters of the model are adjusted and the process is repeated until some criteria are met.
In addition to pre-training, the model can also be fine tuned to allow for even better results \citep{minaee_large_2025}.

There are countless additional details that can be studied about large language models and their training process.
For the purpose of this thesis however, this brief explanation should suffice to understand the LLM as a recent technology capable of generating coherent text:
If it can generate text on a wide variety of topics after all, why wouldn't it be able to generate text representing navigational instructions as well?

GPT-3 released by OpenAI in 2020 is considered by many to be the first example of a large language model \citep{minaee_large_2025}.

% What are emerging capabilities of LLMs?

\section{Emerging Abilities of LLMs}

Beyond mere language understanding and generation, LLMs exhibit further exciting abilities enabled by their large scale, namely emerging abilities.
Emerging abilities describe a set of abilities LLMs have displayed, while not being explicitly trained for these abilities.
The defining characteristic of emerging abilities is that they are not simply a scaled up version of smaller models' abilities.
Rather, they only appear once a certain model size is reached.
Examples for these abilities include in-context learning or reasoning abilities.
In-context learning allows LLMs to learn new tasks based on a few new examples provided in the input, without any additional training.
Reasoning abilities describe the ability of LLMs to solve problems that require multiple sequential steps to arrive at a solution, for example in the domain of math \citep{wei_emergent_2022}.

These emerging abilities are of particular interest to our study.
Solving navigational tasks may require not only the mere generation of text describing the route, but also an understanding of the spatial context to begin with.
With their emerging abilities, LLMs may be able to perform these tasks despite not being explicitly trained for them (as of now we were not able to find any LLMs trained for navigation tasks, but finding LLMs with advertised abilities such as reasoning is easy today).


% Context Enrichment for LLMs

\section{Context Enrichment for LLMs}

In this section, we will discuss the concept of context enrichment for large language models.
Unfortunately, in many cases LLMs have shown to produce incorrect or misleading outputs, a phenomenon often referred to as "hallucination".
Although the content of a hallucination may seem plausible at first glance, it is often factually incorrect \citep{huang_survey_2025}.
The navigation example given in the introductory chapter of this thesis can be considered as an example of such a hallucination:
Although it appeared like a sensible set of directions to get from start to finish, under closer inspection the directions could be shown to contain many errors.
The initial plausibility and semantic correctness are the key differences between a hallucination and random, degenerate output \citep{huang_survey_2025}.
To mitigate the frequency of hallucinations, researchers have come up with techniques to provide additional information to the LLM after the training process is complete \citep{minaee_large_2025}.

In practice, this is usually done by enriching the LLMs' context.
LLMs possess a finite context window, within which they can attend to tokens.
Traditionally, this context window would contain the chat history of the session as well as optional system prompts.
With context enrichment techniques however, additional information can be included in the context window \citep{minaee_large_2025}.

Retrieval augmented generation (or RAG) is one such technique.
With RAG, an additional external knowledge base is connected to the LLM system.
This external knowledge base can contain arbitrary sets of information, for example company documents or other niche specific knowledge which may be underrepresented in the training data.
When a user prompts the LLM, the system first queries the external knowledge base.
This query is directly derived from the user prompt.
Relevant snippets of the external knowledge base are then appended to the LLMs' context window.
Finally, after the context window has been enriched with this additional information from the knowledge base, the LLM can start generating its output \citep{lewis_retrieval-augmented_2021}.

The effects of RAG on LLM hallucinations have been studied in previous research.
In knowledge-intensive tasks such as question answering, RAG has been shown to substantially reduce hallucinations \citep{gao_retrieval-augmented_2024}.
This may serve as a first indication that the technique could be useful for our study:
If we consider navigation tasks to be knowledge-intensive as well, RAG could potentially help LLMs generate accurate navigation instructions.

RAG has been implemented in various ways.
One specific RAG implementation relevant to our topic is graph RAG.
In this approach, the external knowledge base is not simply a collection of text chunks, but rather a graph structure.
This graph structure typically represents relationships between entities by modeling them as nodes and edges.
Similar to traditional RAG, the user prompt is used to query the graph structure for relevant nodes and edges.
These relevant nodes and edges are then converted to text snippets and appended to the LLM's context window.
In some cases, the graph structure may first have to be constructed from text, while in other cases a graph may be present in the first place \citep{edge_local_2025}.

Examples of graph structures include knowledge graphs or social networks, as well as road networks.
For our work this opens the possibility for a graph RAG system designed for improved LLM navigation as a long term goal.
In this thesis however, we will not implement such a system, but provide the additional geographic context directly within the prompt, emulating the effect of a RAG system.

% While a graph rag pipeline is a long term goal for the academic efforts surrounding this study, in our case we will simply emulate the effect by providing additional context in the prompt.

% What is topological geographic data and why is it relevant to this study.

\section{Qualitative Geographic Data}

As we have seen, LLMs possess the ability to generate coherent sequences of text \citep{minaee_large_2025}and their emerging abilities open the door to solving more complex tasks as well \citep{wei_emergent_2022}.
Additionally, by using context enrichment techniques such as RAG, unwanted effects like hallucinations can be mitigated \citep{gao_retrieval-augmented_2024}.
In this thesis, we want to explore the potential of these techniques in the domain of navigation by testing whether LLM navigation can be improved by providing additional geographic context.
The missing piece of the puzzle is thus a suitable representation of geographic data which can then be provided to the LLM.

Geographic data can be represented in many ways.
A key distinction can be made between qualitative and quantitative geographic data.
While quantitative geographic data contains precise measurements such as distances or coordinates, qualitative geographic data describes properties and relationships of geographic entities \citep{lwin_quantitative_2012}. 

\begin{table}[h]
    \centering
    \begin{tabular}{p{0.25\textwidth} p{0.3\textwidth} p{0.35\textwidth}}
        \hline
        \textbf{Aspect} & \textbf{Qualitative representation} & \textbf{Quantitative representation} \\
        \hline
        Description &
        ``Examplestreet'' starts at the intersection with ``Teststreet''. \newline
        ``Examplestreet'' ends at the intersection with ``Endstreet''. &
        Name: Examplestreet \newline
        Length: 300 m \newline
        Width: 6 m \newline
        Starting coordinates: (1,1) \newline
        End coordinates: (6,7) \\
        \hline
    \end{tabular}
    \caption{Example of qualitative and quantitative representations of a street.}
    \label{tab:qual_quant_road}
\end{table}

In the example given in Table 2.1, we demonstrate the difference between qualitative and quantitative geographic data in the context of our research topic.
While the quantitative representation focuses on precise numbers such as "Length: 300 m", the qualitative representation describes the same road purely based on qualitative descriptions.
This does not mean that the two descriptions have the same information content, or that one representation is better than the other, however; both descriptions follow a unique paradigm and are suited for different tasks.
We hope that the qualitative representation may be better suited for LLM digestion, since as the example shows, qualitative geographic data can be represented nicely using natural language, the native medium of LLMs \citep{minaee_large_2025}.

So far, we have only seen a small example of qualitative geographic data.
In the end, we want to represent entire street networks in a qualitative manner.
To achieve this, a framework capable of representing topological data qualitatively is necessary.
Next, we will present such a framework, called the dipole calculus.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{chapter/dipole_relations.png} % Adjust width as needed
    \caption{All qualitatively different dipole relations that are necessary to describe a street network.}
    \label{fig:graphic3}
\end{figure}

The dipole calculus is a qualitative spatial reasoning framework.
In the framework, elongated, oriented objects (such as road segments) are represented as directed line segments with a start and end point, called dipoles.
Pairing two of these dipoles together results in a relation that can be classified into one of several relational categories.
The set of these relational categories is finite.
Examples include relations where one dipole continues another, crosses it, or branches off from it.
This is an abstraction of the real world, and coordinates or distances are not represented in this framework.
In the end, this results in a framework which allows for qualitative reasoning on line-based geographic structures such as road networks \citep{moratz_condensed_2011}.

Although extended versions of the dipole calculus exist, all necessary dipole relations for our study are shown in Figure 2.1.
In the Figure, each dipole is represented by a unidirectional arrow.
Each dipole has a unique label (A-N) for identification.
For example, we see that dipole G branches off left from dipole B.
We can also see, that dipole A continues straight onwards from dipole B, and dipole E continues straight onwards from dipole A.
In practice, this means that a street may be broken down into several dipoles using this approach.
The mapping between dipoles and streets is thus not necessarily a strict one to one relationship:
While a dipole is always mapped to a single street, a street may be mapped to several dipoles.

\section{Data sources and GIS Tools}

Another key pillar for our research will be the identification of a reliable data source for street networks, which can be transformed into dipole relations.
One such data source is OpenStreetMap (OSM).
OpenStreetMap was created in 2004 and is a community driven, collaborative project providing free geographic data \citep{haklay_openstreetmap_2008}.
Because it is not a commercial product, OpenStreetMap relies on Volunteered Geographic Information (VGI) provided by citizens.
This has led to the term "Citizen Sensor" being introduced to describe the means of data collection \citep{university_of_nottingham_gb_mapping_2017}.
Although this approach has benefits like removing corporate interests from the data, it also has downsides.

Since the data is provided by volunteers, the data quality may vary depending on the region of interest.
Some studies have investigated the data quality of OSM.
A metastudy conducted in 2013 synthesized several of these studies on OSM data quality. 
To summarize, the study found that OSM data was rapidly growing.
While the overall data quality was found to be heterogeneous, meaning that it varied from region to region, the data quality was assessed to have improved over time \citep{sehra_assessment_2013}.
In an article published in 2014, researchers specifically compared the OSM road data available for Germany to proprietary datasets and found a 9\% difference in total network length, indicating that the OSM dataset contained relatively few gaps in the investigated region \citep{neis_recent_2014}.
The same article also notes that many studies mention an urban bias in OSM data, meaning that more densely populated areas tend to have better data coverage than rural areas \citep{neis_recent_2014}.
While this may be a limitation for other use cases, for our study this may not be a major issue, since we are investigating navigation tasks in urban scenarios, where the coverage reportedly tends to be better.

Even though we cannot ignore the fact that OSM data quality may vary, some researchers have shown that it is a robust choice for practical implementations.
In a conference paper from 2011, researchers investigated the use of OSM data for real time routing.
The researchers successfully demonstrated that OSM data was a suitable choice for both server side and handheld routing implementations \citep{luxen_real-time_2011}.
For us, this indicates that OSM data may be a suitable choice for our research regarding improved LLM navigation, especially since we are mainly focused on urban environments.

Many studies analyzing or utilizing OSM data have suffered from poor reproducibility and small sample sizes.
To address these issues, a research initiative called OSMnx was brought to life in 2017.
OSMnx provides an open source Python library, which allows users to easily download and visualize street networks from OpenStreetMap \citep{boeing_osmnx_2017}.
In this study, we will make use of OSMnx to obtain the street networks we require for our experiments.

Further, we will require a reliable tool to validate the generated navigation instructions.
Since the validation in this study will be done manually, we need a tool with a user-friendly interface and reliable data coverage.
Also, because we do not want to validate the instructions on the same data source we obtained the street networks from, we will not use OpenStreetMap for this purpose.
Instead we will use a tried and tested commercial product called Google Maps.
Following this approach separates the data used in our method from the data used in the validation process, which reflects a key principle in model validation \citep{steyerberg_prediction_2016}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/google_maps_interface.png} % Adjust width as needed
    \caption{A screenshot demonstrates the Google Maps web interface, map data © Google.}
    \label{fig:graphic4}
\end{figure}

Google Maps is a web based mapping platform originating from the Australian company Where 2 Technologies, which was fully acquired by Google in 2004 \citep{noauthor_google_2025}.
In a recent report from 2020, Google, the company behind Google Maps, claims that the platform counts over 1 billion monthly active users \citep{noauthor_look_2020}.
Additionally, a survey conducted on 511 smartphone users in 2018 found that 67\% of respondents used Google Maps as their primary navigation app \citep{noauthor_popularity_nodate}.

In 2019 Google claimed that the data they use for Google Maps was sourced from over 1000 third party services globally \citep{noauthor_google_2019}.
Although many other competing commercial products such as Apple Maps, released in 2012 \citep{noauthor_apple_nodate}, exist in the modern app landscape, Google Maps, backed by its long history and widespread popularity, will be the tool of choice for the validation process in this study.

By discussing LLMs, their emerging capabilities such as reasoning, context enrichment techniques, a framework for qualitative geographic representations and our choice of data sources, we have now discussed the key ingredients for our research.
What is left to do before moving on to the description of our method, is to review existing research on LLM benchmarks.

\section{LLM Benchmarks and Opportunities}

Exploring LLM Benchmarks is a vast topic.
Since we are interested in creating our own benchmark for LLM navigation tasks, it is important for us to take a look at the existing landscape however.
Therefore, in this section we will discuss how LLMs were historically benchmarked, and whether any existing benchmarks take navigation problems into consideration.

Some of the historical benchmarks for language models include GLUE \citep{wang_glue_2019} and SuperGLUE \citep{wang_superglue_2020}.
In GLUE, a model is evaluated on a set of nine different NLU (natural language understanding) tasks such as sentiment analysis or textual entailment.
Sentiment analysis describes the task of classifying a given text by its sentiment.
Textual entailment on the other hand describes whether one text (a hypothesis) is supported by another (a premise).
For each of the nine tasks, a model produces predictions, and the individual task scores are aggregated into a final GLUE score.
The GLUE score could then be used as a single metric to compare different models \citep{wang_glue_2019}.

A study conducted in 2019 showed that non-expert human annotators outperformed models on six of nine GLUE tasks, achieving an average score of 87.1 compared to then state-of-the-art (and fine tuned) models scoring 83.9 \citep{nangia_human_2019}.
The headroom began to close however, and in 2020 SuperGLUE was introduced to provide more challenging tasks \citep{wang_superglue_2020}.

While GLUE and SuperGLUE provided standardized evaluation for the NLU capabilities of models, they were less suited to measuring the broader capabilities that LLMs began to exhibit.
To address this gap, new benchmark suites were proposed, including BIG-bench (Beyond the Imitation Game) introduced in 2022.
The BIG-bench paper describes a benchmark of over 200 tasks, contributed by 450 researchers across 132 institutions.
The included tasks span a diverse range of topics such as mathematics, physics and software development, shifting evaluation from NLU toward more general problems.
A human baseline score was established, and in addition model scale was taken into account during performance evaluation \citep{srivastava_beyond_2023}.

Another shift in model evaluation could be observed with the introduction of HELM (Holistic Evaluation of Language Models) in 2022.
Language models were increasingly used as the basis for real-world applications, and this called for additional evaluation dimensions.
Helm introduced a framework to evaluate model responses not only on their accuracy, but also on aspects such as fairness, robustness and efficiency.
Fairness describes whether model responses differ systematically between certain groups.
With robustness, a measure was introduced to evaluate whether a model's performance degrades when faced with adversarial inputs.
The computational resources required to run a model were evaluated using the efficiency metric \citep{liang_holistic_2023}.

Beyond mere benchmark suites, evaluation practices have also shifted toward reusable infrastructure.
An example of this is OpenAI's Evals framework.
The framework is an open source registry of existing evaluation suites, as well as tools to create new evaluations \citep{noauthor_openaievals_2025}.

In addition to benchmark suites and evaluation frameworks, crowd-sourced model evaluation methods have also emerged.
The web based platform LMArena (formerly Chatbot Arena) enables users to compare two models' responses side-by-side in anonymous pairwise "battles".
These human preference evaluations are then aggregated into a leaderboard \citep{chiang_chatbot_2024}.

Together, these developments show that the field of LLM evaluation has evolved to account for different factors:
While benchmarks such as GLUE and SuperGLUE were used to benchmark models' NLU capabilities \citep{wang_glue_2019,wang_superglue_2020}, benchmark suites such as the BIG-bench aimed to evaluate the broader capabilities of language models \citep{srivastava_beyond_2023}.
Since language models increasingly found their way into real-world applications, evaluation frameworks like HELM introduced additional metrics \citep{liang_holistic_2023}.
Frameworks such as OpenAI's Evals allowed for reusable evaluation infrastructure \citep{noauthor_openaievals_2025}.
Finally, to incorporate human preference signals into model evaluation, crowd-sourced platforms like LMArena have emerged \citep{chiang_chatbot_2024}.

While the previously presented LLM benchmarks are impressive and cover a wide array of topics, they do not target navigation tasks specifically.

In a paper published in 2024, researchers introduced a benchmark called MANGO that evaluates mapping and navigation in maze environments.
In their trials the LLMs were provided with textual walkthroughs covering the maze environments, and subsequently asked to answer navigation questions about these mazes.
The authors report that models including GPT-4 struggled with the navigation tasks, while humans achieved high accuracy on the problems \citep{ding_mango_2024}. 

A paper from 2022 illustrated the shortcomings of LLMs in planning tasks.
Several prominent LLMs were evaluated on a suite of action and change reasoning tasks.
Across these planning tasks, the models achieved very low success rates.
The models struggled even more to produce optimal results \citep{valmeekam_large_2022}.
Since navigation tasks can be considered as planning tasks, this study can be seen as evidence that LLMs may struggle with navigation.

In 2025, researchers investigating recent ‘reasoning’ variants of LLMs identified a sharp decline in performance as problem complexity increased.
They demonstrated these effects in controllable puzzle environments, where both reasoning and non-reasoning models ultimately collapsed to near-zero accuracy.
A counterintuitive discovery was made as well:
As problem complexity increased, the models initially increased their ‘thinking' effort;
however, after a certain point, reasoning traces became shorter again, even though token budget was still available \citep{shojaee_illusion_2025}.
For our navigation tasks, this could indicate that LLMs could be able to solve simpler navigation problems, but may fail if the problems become too complex.

\section{Summary}

We began this chapter by introducing large language models and their emerging abilities.
Next, we discussed context enrichment techniques such as retrieval augmented generation and one specific implementation called graph RAG.
Afterwards, we presented how geographic data can be represented qualitatively using the dipole calculus.
Thereafter, we discussed our choice of data sources and GIS tools.
Finally, we reviewed existing research on LLM benchmarks.
Prior benchmarks on LLMs show that they struggle with tasks requiring planning \citep{valmeekam_large_2022}.
Some research has also identified that even more recent reasoning variants struggle with complex problems \citep{shojaee_illusion_2025}.
While navigation tasks have been studied in maze environments \citep{ding_mango_2024}, real-world street-network navigation remains comparatively unexplored in existing LLM benchmarks.
To the best of our knowledge there is also no research on whether qualitative dipole relations can be used to improve LLM navigation performance.
This motivates an opportunity to create a benchmark for LLM navigation tasks using real-world problems and to evaluate whether qualitative geographic context offers significant performance improvements.