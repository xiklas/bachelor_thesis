\chapter{Background}
\label{ch:02background}

% Chapter Introduction
% Scope & Structure

This chapter provides the technical and theoretical background on the topic of navigation using large language models as well as on related topics which are essential for this thesis.
We will first discuss the general capabilities of LLMs, before narrowing down on their performance in spatial reasoning. Afterwards, we will review research on qualitative representations of topological data as well as context enrichment techniques for LLMs and observe if these techniques have been applied to study LLM performance in navigation tasks.


% What is an LLM?
\section{Large Language Models}

The large language model, commonly abbreviated as "LLM", is the central technlology this study revolves around.
While the history of LLMs is a fascinating topic in its own and can be traced back to first attempts at language modeling in the 1950s \citep{minaee_large_2025}, it is to extensive to be covered in this thesis, and also not what our research is about.
Therefore, we are going to focus on a purely practical overview on LLMs and their relevant characteristics to this study.

Large language models are the latest development and a specific subclass of language models in general \citep{minaee_large_2025}.
In practice, the challenge large language models aim to adress is the prediction of the next token in a given sequence of tokens \citep{minaee_large_2025}.
For example, given the input tokens (tokens can be thought of as words or parts of words) "Roses are", a large language model would attempt to predict the next token, which in this case could be "red".
Adding the predicted token to the input sequence, the model could then attempt to predict the next token and so on.
This process is known as autoregressive generation and applicable in other domains such as computer vision as well \citep{xiong_autoregressive_2025}.
To continue with the previous example, the next tokens after "Roses are red" could be "violet", with the prediction process eventually resulting in the sentence "Roses are red, violets are blue".

Creating a model that is able to perform this kind of token prediction and deliver acceptable results is by no means a trivial endeavor.
In fact, one of the biggest challenges with earlier language models such as RNNs (recurrent neural networks) was that they were often unable to maintain cohesion over longer sequences of tokens, making them unsuitable for practical use cases \citep{johnston_revisiting_2025}.


Modern large language models mitigate this issue by making use of several key enabling factors, the first one we will discuss is the transformer architecture.
The transformer architecture allows every token in a sequence to attend every other token during token prediction, enabling coherence over much longer sequences.
This architecture itself is realised through self-attention layers in the model architecture \citep{vaswani_attention_2017}.
All experiments conducted in this thesis are therefore partially enabled by this breakthrough architecture, without which the generation of lengthy, coherent text sequences using the LLMs we employ today would probably not be possible. 

Another key factor driving large language models is the availability of sufficient training data.
To achieve high performance, LLMs need to be trained on vast amounts of text data.
This data is usually scraped from books or the public internet.
Although exact figures on training dataset sizes are difficult to obtain for current models, even early models were trained on datasets containing billions of tokens \citep{minaee_large_2025}.

In the pre-training process, the model is trained using the training data, often to minimize prediction error.
This means that the model, consisting of a substantial number of parameters (often billions) is adjusted iteratively.
In each iteration, the model makes predictions and the prediction error is calculated.
Using this error, the parameters of the model are adjusted and the process is repeated until some criteria are met.
In addition to pre-training, the model can also be fine tuned to allow for even better results \citep{minaee_large_2025}.

There are countless additional details that can be studied about large language models and their training process..
For the purpose of this thesis however, this brief explanation should suffice to understand the LLM as a recent technology capable of generating coherent text:
If it can generate text on a wide variety of topics after all, why wouldn't it be able to generate text representing navigational instructions as well?

GPT-3 released by OpenAI in 2020 is considered by many to be the first example of a large language model \citep{minaee_large_2025}.

% What are emerging capabilities of LLMs?

\section{Emerging Abilities of LLMs}

Beyond mere language understanding and generation, LLMs exhibit further exiting abilities enabled by their large scale, namely emerging abilities.
Emerging abilities describe a set of abilities LLMs have displayed, while not being explicitly trained for these abilities.
The defining characteristic of emerging abilities is that they are not simply a scaled up version of smaller models' abilities.
Rather, they only appear once a certain model size is reached.
Examples for these abilities include in-context learning or reasoning abilities.
In-context learning allows LLMs to learn new tasks based on a few new examples provided in the input, without any additional training.
Reasoning abilities describe the ability of LLMs to solve problems that require multiple sequential steps to arrive at a solution, for example in the domain of math \citep{wei_emergent_2022}.

These emerging abilities are of particular interest to our study.
Solving navigational tasks may require not only the mere generation of text describing the route, but also an understanding of the spatial context to begin with.
With their emerging abilities, LLMs may be able to perform these tasks despite not being explicity trained for them (as of now we were not able to find any LLMs trained for navigation tasks, but finding LLMs with advertised abilities such as reasoning is easy today).


% Context Enrichment for LLMs

\section{Context Enrichment for LLMs}

% While a graph rag pipeline is a long term goal for the academic efforts surrounding this study, in our case we will simply emulate the effect by providing additional context in the prompt.

% What is topological geographic data and why is it relevant to this study.

\section{Topological Geographic Data}

% Introduction to topological geographic data.

Our study heavily relies on the concept of topological geographic data, which has been widely used and investigated already.
In this section we will introduce the concept and explain its relevance to our research.
To understand topological data, we may look at the following example of a subway map:

% Subway Map Figure.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/subway_map.png} % Adjust width as needed
    \caption{Arbitrary section of a london tube map.}
    \label{fig:pasted_graphic}
\end{figure}

% Explanation of Subway Map Figure and its relevance to topological data.

In Figure 1.3, we present an arbitrary cutout of the London tube map, containing sections of various lines such as the central line (red), the circle line (yellow) or the district line (green).
Besides the set of lines, the map also contains a set of stations such as "Westminster" or "Embankment" as well as the famous "Oxford Circus" station.
The map can thus be thought of as a set of nodes (stations) and edges connecting the nodes (lines).
Apart from this, the map does not contain any further information such as concrete distances between stations or their exact geographic locations in terms of latitude and longitude.

% What is the benefit of topological geographic data?

The key takeaway here is that the map serves the simple purpose of allowing users of the tube network to navigate from one station to another, by highlighting their connections and abstracting away anything else.
This abstraction allows for a very clean and efficient representation of otherwise complex networks.
It has been shown that such topological representations significantly aid human navigation in various scenarios.
Because of this, in many cases around the world such as public transport networks, topological representations like the tube map are used in practice.

% What does this have to do with our research?

Topological representations of geographic data are not limited to public transport networks however.
In fact, many other use cases for topological data exist.
For our research, we are particularly interested in road networks, which can also be represented in a topological manner.
To achieve a topological representation of a road network, we simply switch out stations for intersections and lines for roads connecting these intersections.
Again, we abstract away any further information such as distances or exact geographic coordinates.
What we are left with is a simplified representation of the initial network.
We suggest that using this simplified representation may aid LLMs in solving navigation tasks.

% Section on Dipole Relations of topological data.

\section{Dipole Representations of Topological Data}

% Intro on Dipole Relations.

As we have seen in the previous section, we can represent geographic data in the form of topological networks, describing connections between nodes.
In this section, we will present the technique of qualitatively describing such topological data using dipole relations.

% Figure on the dipole relations needed for our study.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/dipole_relations.png} % Adjust width as needed
    \caption{All qualitatively different dipole relations necessary to describe a street network.}
    \label{fig:pasted_graphic}
\end{figure}

% Explanation of Dipole Relations Figure.

By using dipole relations, an entire graph structure can thus be represented in an elegant way using natural language by simply aggregating qualitative statements like "Street A starts at Intersection of B,C and G" or "Street F branches off to the left from Street A".
Dipole relations have been successfully implemented in the field of robotics, where they were used to allow robotic agents to navigate through unknown environments.

% Section on Current Navigation Systems.

\section{Current Navigation Systems}

% Intro on Current Navigation Systems.

Navigation systems have been around for a long time.
For the average consumer, GPS based systems like Google Maps or Apple Maps to name a few, have become the norm when traveling by foot, bike or car.

% Section on Navigation as a Spatial Reasoning Task.

\section{Navigation: A Spatial Reasoning Task}

Since we are dealing with navigation tasks in this thesis, it is interesting to take a look at the abilities usually deemed necessary to successfully engage in the activity of navigation.

% Section on the reasoning capabilities of LLMs.

\section{Reasoning Capabilities of Large Language Models}

% Intro on LLMs.

The entire history of large language models is too vast and extensive to be covered in its entirety in this thesis.
However, this should not prevent us from giving a brief overview of the technology here.
We also aim to explain which of their characteristics leads us to believe that they could be capable of handling the navigation tasks we will confront them with later on.

% Where do LLMs come from?

The origins of LLMs lie in the field of natural language processing (NLP), where they were initially developed to perform tasks such as text generation and translation.
Although the specific architectures of these models have evolved since their inception, the underlying principles have remained rather consistent: LLM's are trained on vast amounts of text data (usually scraped from the internet) to learn patterns and relationships within the training data. Subsequently, if training was successful, the attained models, consisting of millions or even billions of parameters, are then able to generate seemingly coherent text by predicting the next words (or more accurately, tokens) in a sequence.

% What are the most well knwon LLMs?

Some of the most well known LLMs today include OpenAI's GPT series (which first brought the technology to the market) or Google's Gemini models. It is no secret however, that since the 2020s, a large number of competitors have entered the market with their own proprietary models.

% What was the initial use case for LLMs?

As mentioned, the primary potential for LLMs was initially identified in the field of natural language processing.
With the steady increase in training data and model size however, new capabilities began to emerge.
As early as 2021, researchers observed that LLMs were able to pass standardized tests such as the SAT (a test used for college admissions in the United States of America) or GRE (a similar test used for admissions to graduate schools), despite not being explicitly trained for these tasks.
Although previous artifical intelligence systems had already been able to perform well on specific tasks, such as playing chess or even eventually beating human players at the game of Go,, LLMs were thus the first models to demonstrate a more generalized ability to solve problems across a wide range of domains.

% What happened next (reasoning capabilities)?

Since then, the development of LLMs more capable than the original models has continued and no end is in sight. The increase in these models' abilities can be attributed to several factors such as larger training datasets, higher context windows and even more advanced architectures.
It didn't take much longer until big tech companies began to advertise their models with claims that their models were capable of reasoning, and thus being more capable of solving problems in domains such as math, programming and logic.
The term LRM (large reasoning model) was even introduced to describe models with such capabilities.
While certainly a valuable selling point, the actual reasoning capabilities of LLMs have been a topic of debate among scholars ever since.
This study will not attempt to settle this debate in one way or another, but is instead concerned with the practical abilities LLMs showcase when confronted with rather complex problems.

% How have their reasoning capabilites been studied?

To study their abilities when faced with reasoning tasks, researchers have come up with various benchmarks.
One of the most well known benachmarks is the FrontierMATH benchmarks

...

% What is the summary of LLM reasoning capabilities?

To summarize, while their abilities in reasoning tasks are by no means perfect, recent LLMs have proven to be more capable than ever before, and the trend is likely to continue in the foreseeable future.
This opens the door to confront LLMs with spatial reasoning tasks such as navigation, which will be the topic of the upcoming section.

% Section on LLM Spatial Reasoning Capabilities.

\section{LLMs in Spatial Reasoning and Navigation}

% Intro on LLMs in Spatial Reasoning and Navigation.

As we have seen, LLMs abilities to perform complex tasks requiring the use of a process similar to multi step reasoning has increased without any signs of slowdown over the past five years.
Although we cannot infer from this fact alone that LLMs are capable of solving issues in spatial reasoning as well, these new abilities may hint at their use in the field of geographic information science (GIScience).
While the discipline has identified several promising applications for LLMs in the domain, not all the research has concerned itself with their abilities regarding navigation tasks.

% What has been done so far?

Among several other applications, one paper published in 2025 by X et al. highlights the potential of LLMs to serve as natural language interfaces for GI systems (Geo-Information Systems):
In summary, LLMs could be used to allow users to interact with spatial data in new ways by using human language, thereby democratizing access to sophisticated GIS analysis.

If we want to apply this concept to navigation tasks however, we need to accept that LLMs have been shown to perform rather poorly in this area:
In a study conducted by Y et al. in 2025, several LLMs were tested on navigation tasks and their performance could be shown to be lacking.
While the exact reasons for LLMs shortcomings in this area are not part of this thesis, we theorize that a lack of qualitative geographic descripitions in the training data used to train these LLMs may be a significant factor.

% What is the summary on LLM Spatial Reasoning abilities?

While it was proposed that LLMs could serve as natural language interfaces, and their navigational capabilities were shown to be lacking, little work has been done to find ways to alleviate their shortcomings in this regard.
Other work focuses on the use of LLMs to create fully autonomous navigation agents using computer vision and robotics, which - although certainly impressive - is outside the scope of this thesis.
Despite the increase in reasoning capabilities of LLMs in other domains, which were presented in Chapter 1.2.1, using LLMs for navigation tasks seems to be a dead end as of today.
In the next section, we will present a proven technique to enhance LLM performance in various domains.

% Section on Context Enrichment Techniques for LLMs.

\section{Context Enrichment Techniques for LLMs}

% Intro on Context Enrichment Techniques for LLMs.

After several studies presented convincing evidence concerning hallucinations and other shortcomings of LLMs, researchers began to look for possible techniques to reduce the occurence of such issues. 
With context enrichment techniques, one promising approach was quickly identified:
To account for blind spots in the datasets used to train the LLMs, researchers began to allow LLMs to access additional knowledge when presented with a user query.
