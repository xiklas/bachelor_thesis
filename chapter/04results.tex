%!TEX root = ../thesis.tex
\chapter{Results}
\label{ch:04results}

This chapter serves to present the results of our experiments.
In total, 240 individual LLM responses were collected across various configurations to find out whether the inclusion of qualitative geographic context helps improve the navigation performance of LLMs.
We are first going to provide an overview of the results obtained from our experiments, before breaking down the results by different factors such as test city or large language model tested.
These results will then be summarized and serve as the basis for the discussion following in the Chapter 5.

\section{Dataset Overview}

\begin{table}[h!]
\centering
\begin{tabular}{l r r}
\hline
\textbf{City} & \textbf{\# Individual Streets} & \textbf{Avg. Task Distance (m)} \\
\hline
Hamburg & 38 & 899 \\
Münster & 128 & 1272 \\
\hline
\end{tabular}
\caption{Overview of test datasets.}
\label{tab:dataset-overview}
\end{table}

Table~\ref{tab:dataset-overview} provides an overview of the two datasets used in our experiments.
We were able to successfully derive symbolic dipole relations for both tested cities.
For the Hamburg dataset, this resulted in a total of 38 individual streets and an average start-end distance of 899 meters.
On the other hand, the Münster dataset contained a total of 128 individual streets, and also had a higher average start-end distance of 1272 meters (both measured using Google Maps).


\section{Experiment Results}

\begin{table}[h!]
\centering
\begin{tabular}{l r r r r}
\hline
\textbf{Group} & \textbf{\# Trials} & \textbf{\# Successful} & \textbf{\# Failed} & \textbf{Success Rate (\%)} \\
\hline
Control Group & 120 & 0  & 120 & 0 \\
Test Group    & 120 & 75 & 45  & 62.5 \\
\hline
\textbf{Total} & \textbf{240} & \textbf{75} & \textbf{165} & \textbf{31.25} \\
\hline
\end{tabular}
\caption{Overview of trial outcomes in control and test groups with success and failure counts and corresponding success rates.}
\label{tab:trial-outcomes-overview}
\end{table}


% Could all data be collected successfully?

Table~\ref{tab:trial-outcomes-overview} provides an overview of the trial outcomes.
All trials could be conducted successfully by the methods described in Chapter 3. 
This means that we got a valid LLM response for each navigation task under all test conditions.
However, this does not imply that all responses were correct in terms of navigation success, but simply that we were able to collect all the necessary data.
In total, 240 LLM navigation responses (trials) were collected and manually labeled as either correct or incorrect.

% What is the total performance gain between control and test group?

There is a visible difference in navigation success rates between the control and test groups.
In each group, the exact same 120 navigation trials were conducted.
Out of these 120 trials, no LLM response in the control group could be labelled as successful.
Consequently, all trials in this group were labelled as failures, leading to a success rate of 0\%.
However, in the test group, 75 out of 120 trials could be labelled as successful.
With 45 remaining failures, this results in a success rate of 62.5\%.
Across both groups, this means that 75 out of 240 navigation trials were labelled as successful, while 165 trials were labelled as failed.
This leads to an overall success rate of 31.25\% across all experiments conducted in this thesis.
To summarize, the inclusion of qualitative geographic context increased the success rate by 62.5 percentage points when compared to the control group.

% Comparing Test Group Performance By City

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/diagrams/success_rate_by_city.png} % Adjust width as needed
    \caption{Test Group success rate by City.}
    \label{fig:success-rate-by-city}
\end{figure}

Figure~\ref{fig:success-rate-by-city} illustrates the success rates in the test group broken down by the two tested cities.
When comparing test group performances under this condition, we can make several observations.
First, the success rate increased in both cities compared to the control group (which had a success rate of 0\% in both cities).
In Hamburg, the success rate climbed to 86.6\%, while in Münster it reached just 38.3\%.
This means that the success rate in Hamburg was more than double that of Münster.
Out of the 75 correctly labelled navigation trials in the test group, 52 were conducted using the Hamburg dataset, while 23 were conducted using the Münster dataset.

% Breakdown Per Model

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/diagrams/success_rate_by_model.png} % Adjust width as needed
    \caption{Test group success rate by LLM.}
    \label{fig:success-rate-by-model}
\end{figure}

Further, comparing the success rates across the three tested LLMs in the test group as depicted in Figure~\ref{fig:success-rate-by-model} yields additional insights. 
While all models performed better compared to their control group success rate of 0\%, the best-performing model in the test group was Google's Gemini 2.5 Pro.
It achieved a success rate of 70\%, with 28 out of 40 navigation trials labelled as correct.
The next best-performing model was Claude Sonnet 4.5 with a slightly lower success rate of 65\%.
Claude answered 26 out of 40 navigation trials correctly.
GPT-4o performed worst in the test group: with 21 out of 40 navigation trials labelled as correct, it achieved a success rate of 52.5\%.

% Breakdown per city and model

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/diagrams/success_rate_by_model_and_city.png} % Adjust width as needed
    \caption{Test group success rate by model and city.}
    \label{fig:success-rate-by-model-and-city}
\end{figure}

Breaking down the test group success rates by both test city and LLM as shown in Figure~\ref{fig:success-rate-by-model-and-city} leads to several additional observations.
First, we can see that all models performed better with the Hamburg dataset compared to the Münster dataset.
All models using the Hamburg dataset achieved a success rate of at least 75\%, with Gemini 2.5 Pro ranking the highest at 95\%.
Claude Sonnet 4.5 followed with a success rate of 90\%, five percentage points below Gemini 2.5 Pro.
GPT-4o achieved a success rate of 75\%, 20 percentage points lower than Gemini 2.5 Pro.
With the Münster dataset, all models performed worse overall, although their relative rankings remained the same.
Gemini 2.5 Pro again ranked highest among the three with a success rate of 45\%, followed by Claude Sonnet 4.5 at 40\% and GPT-4o at 30\%.
The lowest overall success rate in the test group was thus observed with GPT-4o using the Münster dataset at 30\%, while the highest overall success rate was achieved by Gemini 2.5 Pro using the Hamburg dataset at 95\%.

% How many tasks were correctly answered by all models?

\begin{table}[h]
\centering
\begin{tabular}{c c}
\hline
\textbf{Number of Correct LLM Responses} & \textbf{Frequency} \\
\hline
0 & 6 \\
1 & 9 \\
2 & 9 \\
3 & 16 \\
\hline
\end{tabular}
\caption{Frequency distribution of correct LLM responses in the test group.}
\label{tab:llm-frequency-distribution}
\end{table}


We may also examine the frequency distribution of correctly answered navigation tasks in the test group.
Table~\ref{tab:llm-frequency-distribution} summarizes, for each navigation task, how many of the three tested models produced a response labelled as correct.
Only six navigation tasks were answered incorrectly by all three models.
Nine navigation tasks were answered correctly by just one model.
Nine navigation tasks were answered correctly by two models.
This means that the likelihood of all models failing given that at least one model failed was exactly 25\% (the number of cases where all models failed (6) divided by the number of cases where at least one model failed (24)).
The highest frequency could be observed for navigation tasks where all three models answered correctly, with a total of 16.
Conversely, this means that the likelihood of all models answering correctly when at least one model answered correctly was approximately 47.06\% (The number of cases where all models answered correctly (16) divided by the number of cases where at least one model answered correctly (34)).

% Continue Here
% Which model performed best when only one model succeeded?

\begin{table}[h]
\centering
\begin{tabular}{l c}
\hline
\textbf{Model} & \textbf{Frequency} \\
\hline
GPT-4o & 3 \\
Gemini 2.5 Pro & 4 \\
Claude Sonnet 4.5 & 2 \\
\hline
\end{tabular}
\caption{Distribution of correct models in single-success tasks.}
\label{tab:single-success-llm}
\end{table}

Additionally, we analyzed which model performed best in cases where exactly one model provided a response to a navigation task that was labelled as correct.
The results of this analysis are summarized in Table~\ref{tab:single-success-llm}.
Although there were only nine such cases overall and the spread between the best- and worst-performing model was small, Gemini 2.5 Pro performed best in this category.
It was able to provide a correct answer in four cases when GPT-4o and Claude Sonnet 4.5 both failed.
GPT-4o then followed with three such cases.
Claude Sonnet 4.5 performed worst, answering two navigation tasks correctly when both other models failed. 

% When just one model failed, which model was most likely to fail?

\begin{table}[h]
\centering
\begin{tabular}{l c}
\hline
\textbf{Model Failed} & \textbf{Frequency} \\
\hline
GPT-4o & 7 \\
Gemini 2.5 Pro & 1 \\
Claude Sonnet 4.5 & 1 \\
\hline
\end{tabular}
\caption{Distribution of single failed model in two-success trials.}
\label{tab:single-failed-llm}
\end{table}


We also consider the complementary case: for tasks where exactly one model failed, we aim to identify which model was most likely to fail.
The results of this analysis are summarized in Table~\ref{tab:single-failed-llm}.
GPT-4o accounted for seven out of nine such cases, making it the model most likely to fail when both other models succeeded.
Gemini 2.5 Pro and Claude Sonnet 4.5 accounted for exactly one failure of this kind each, so their performance in this category was identical.

% Failures by City

\begin{table}[h]
\centering
\begin{tabular}{l r r}
\hline
\textbf{\# Failed Models} & \textbf{Hamburg} & \textbf{Münster} \\
\hline
1 & 4 & 5 \\
2 & 2 & 7 \\
3 & 0 & 6 \\
\hline
\textbf{Total} & \textbf{6} & \textbf{18} \\
\hline
\end{tabular}
\caption{Distribution of failures by city and number of failed models.}
\label{tab:failures-by-city}
\end{table}


Next, we analyzed the failures by the dataset they occurred on (Hamburg or Münster).
The results of this analysis are summarized in Table~\ref{tab:failures-by-city}.
Cases in which only one model answered incorrectly were distributed almost evenly across both city datasets:
using the Hamburg dataset, there were 4 occurrences where one model failed to answer a navigation task.
Using the Münster dataset, this number was just slightly higher at 5.
For tasks in which exactly two models failed, we counted two such cases in Hamburg and seven cases in Münster.

The largest disparity between both cities could be observed in cases where all three models failed:
No navigation task conducted on the Hamburg dataset resulted in all three models failing to provide a correct answer.
Using the Münster dataset however, there were six such occurrences.

Aggregating across categories, there are 24 total cases where at least one model failed to answer a navigation task correctly.
Out of these, 6 cases, or 25\% occurred using the Hamburg dataset, while the remaining 18 cases, or 75\% occurred using the Münster dataset.

In total, there are 15 total cases where at least two models failed to answer a navigation task correctly.
Out of these, just 2 cases, or 13.33\% occurred using the Hamburg dataset, while the remaining 13 cases, or 86.67\% occurred using the Münster dataset.

Lastly, all of the six instances where all three models failed to answer a navigation task correctly occurred using the Münster dataset.

\begin{table}[h]
\centering
\begin{tabular}{l r r}
\hline
\textbf{Model pair} & \textbf{Disagreement frequency} & \textbf{Agreement frequency} \\
\hline
GPT-4o vs.\ Gemini 2.5 Pro      & 15 & 3  \\
GPT-4o vs.\ Claude Sonnet 4.5   & 13 & 5  \\
Gemini 2.5 Pro vs.\ Claude Sonnet 4.5 & 8  & 10 \\
\hline
\end{tabular}
\caption{Pairwise model disagreement and agreement in non-unanimous trials.}
\label{tab:pairwise-disagreement}
\end{table}


Finally, we analyze pairwise disagreement among the three studied models.
To do so, we exclude all tasks which were answered unanimously, meaning that either all models answered correctly or all models answered incorrectly.
This leaves us with a total of 18 non-unanimous tasks.

Within this subset of 18 tasks, we measure how often each pair of models disagreed.
Disagreement is defined here as one model answering correctly while the other model answered incorrectly (or vice versa).
The results of this analysis are summarized in Table~\ref{tab:pairwise-disagreement}.

Out of the 18 non-unanimous tasks, GPT-4o and Gemini 2.5 Pro disagreed in 15 cases, while agreeing on 3 cases.
The next highest disagreement count was observed in the pairing GPT-4o and Claude Sonnet 4.5, which disagreed in 13 cases and agreed in 5 cases.
With just 8 disagreements and 10 agreements, the pairing of Gemini 2.5 Pro and Claude Sonnet 4.5 showed the lowest amount of disagreements out of all combinations.

Accordingly, we conclude that Gemini 2.5 Pro and Claude Sonnet 4.5 agreed more often than they disagreed.
On the other hand, any combination involving GPT-4o resulted in more disagreements than agreements.

\section{Summary of Findings}

Overall, our experiments indicate that LLM navigation performance with additional symbolic geographic context improved compared to the control group without context in our test setup.
While the control group achieved no successful trials, the control group achieved non-zero success rates across all tested LLMs and datasets.
The magnitude of performance gains varied by the two city datasets: success rates in Hamburg were higher for all models compared to Münster.
Among the tested LLMs, Gemini 2.5 Pro achieved the highes success rate, followed by Claude Sonnet 4.5 and GPT-4o.
These results will serve as the basis for the discussion following in Chapter 5.
