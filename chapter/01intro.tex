%!TEX root = ../thesis.tex
\chapter{Introduction}
\label{ch:01intro}

In this chapter, we introduce the research topic of navigation using large language models.
After briefly discussing the current capabilities of LLMs in navigation tasks, we review the existing body of research in this emergent area.
We then identify the research gap we aim to adress in this thesis and formulate a corresponding research hypothesis.

\section{The Current State of Navigation using LLMs}

In practice, users rely on large language models for an increasing number of tasks.
Starting out as the latest advancements in natural language processing, LLMs have quickly found many other applications such as computer code generation in software development or more creative uses such as image and video synthesis.
Although earlier models possessed were limited in these areas, more recent models such as GPT-4o and Gemini 2.5 Pro have demonstrated impressive performance due to their emerging capabilities such as reasoning.
As demonstrated in the depiction below, these models can also be confronted with navigational tasks like route planning:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/PastedGraphic-2.png} % Adjust width as needed
    \caption{GPT-4o response to a simple route planning task.}
    \label{fig:pasted_graphic}
\end{figure}

Although alternative route planning solutions exist, this example illustrates the potential of using LLMs as purely text-based interfaces for navigational tasks, as compared to traditional map-based implementations such as Google Maps or OpenStreetMap.
For this concept to be viable in practice however, LLMs must provide accurate and reliable responses, as mistakes in navigation can lead to significant problems in many scenarios.
Investigation on the reliability of LLMs in this regard have already been conducted, revealing a concerning tendency to provide infactual information when confronted with route planning scenarios:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/PastedGraphic-3.png} % Adjust width as needed
    \caption{The correct route for the initial task (green) compared to the GPT-4o response (red).}
    \label{fig:pasted_graphic}
\end{figure}

Even though the model response shown in Figure 1.1 appears structurally sound and plausible at first glance, upon further inspection the example demonstrates quite well that LLMs can fail to give accurate answers to navigational tasks when compared to the ground truth (Figure 1.2).
In this case, GPT-4o suggested several wrong and disconnected directions, a striking contrast compared to the actual route.
Additionaly, traversal of a street was suggested that doesn't exist in the investigated city at the time of writing (Willy-Brandt-Allee).

Observations like these have left room for research systematically studying the shortcomings of contemporary LLMs in spatial reasoning, and if the inclusion of qualitative geographic context in the form of natural language dipole relations can significantly improve their performance in this regard. 
We thus conduct a series of experiments to evaluate the navigation capabilities of select LLMs across multiple geographic areas, both with and without the inclusion of qualitative geographic context.
In order to gain a clear picture of the current state of research in this area, we must first review the relevant literature.

\section{Literature Review}

This chapter will serve as a brief overview of the relevant existing literature on the topic of navigation using large language models.
We will first discuss the general capabilities of LLMs, before narrowing down on their performance in spatial reasoning. Afterwards, we will review research on qualitative representations of topological data as well as context enrichment techniques for LLMs and observe if these techniques have been applied to study LLM performance in navigation tasks.

\subsection{Topological Geographic Data}

Our study heavily relies on the concept of topological geographic data, which has been widely used and investigated already.
In this section we will introduce the concept and explain its relevance to our research.
To understand topological data, we may look at the following example of a subway map:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/subway_map.png} % Adjust width as needed
    \caption{Arbitrary section of a london tube map.}
    \label{fig:pasted_graphic}
\end{figure}

In Figure 1.3, we present an arbitrary cutout of the London tube map, containing sections of various lines such as the central line (red), the circle line (yellow) or the district line (green).
Besides the set of lines, the map also contains a set of stations such as "Westminster" or "Embankment" as well as the famous "Oxford Circus" station.
The map can thus be thought of as a set of nodes (stations) and edges connecting the nodes (lines).
Apart from this, the map does not contain any further information such as concrete distances between stations or their exact geographic locations in terms of latitude and longitude.

The key takeaway here is that the map serves the simple purpose of allowing users of the tube network to navigate from one station to another, by highlighting their connections and abstracting away anything else.
This abstraction allows for a very clean and efficient representation of otherwise complex networks.
It has been shown that such topological representations significantly aid human navigation in various scenarios.
Because of this, in many cases around the world such as public transport networks, topological representations like the tube map are used in practice.

Topological representations of geographic data are not limited to public transport networks however.
In fact, many other use cases for topological data exist.
For our research, we are particularly interested in road networks, which can also be represented in a topological manner.
To achieve a topological representation of a road network, we simply switch out stations for intersections and lines for roads connecting these intersections.
Again, we abstract away any further information such as distances or exact geographic coordinates.
What we are left with is a simplified representation of the initial network.
We suggest that using this simplified representation may aid LLMs in solving navigation tasks.

\subsection{Dipole Representations of Topological Data}

As we have seen in the previous section, we can represent geographic data in the form of topological networks, describing connections between nodes.
In this section, we will present the technique of qualitatively describing such topological data using dipole relations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter/dipole_relations.png} % Adjust width as needed
    \caption{All qualitatively different dipole relations necessary to describe a street network.}
    \label{fig:pasted_graphic}
\end{figure}

By using dipole relations, an entire graph structure can thus be represented in an elegant way using natural language by simply aggregating qualitative statements like "Street A starts at Intersection of B,C and G" or "Street F branches off to the left from Street A".
Dipole relations have been successfully implemented in the field of robotics, where they were used to allow robotic agents to navigate through unknown environments.


\subsection{Navigation: A Spatial Reasoning Task}

What are the abilities usually attributed to successful navigation in any agent?

\subsection{Reasoning Capabilities of Large Language Models}

The origins of LLMs lie in the field of natural language processing (NLP), where they were initially developed to perform tasks such as text generation and translation.
Although the specific architectures of these models have evolved since their inception, the underlying principles have remained rather consistent: LLM's are trained on vast amounts of text data (usually scraped from the internet) to learn patterns and relationships within the training data. Subsequently, if training was successful, the attained models, consisting of millions or even billions of parameters, are then able to generate seemingly coherent text by predicting the next words (or more accurately, tokens) in a sequence.
Some of the most well known LLMs today include OpenAI's GPT series (which first brought the technology to the market) or Google's Gemini models. It is no secret however, that since the 2020s, a large number of competitors have entered the market with their own proprietary models.

As mentioned, the primary potential for LLMs was initially identified in the field of natural language processing.
With the steady increase in training data and model size however, new capabilities began to emerge.
As early as 2021, researchers observed that LLMs were able to pass standardized tests such as the SAT (a test used for college admissions in the United States of America) or GRE (a similar test used for admissions to graduate schools), despite not being explicitly trained for these tasks.
Although previous artifical intelligence systems had already been able to perform well on specific tasks, such as playing chess or even eventually beating human players at the game of Go,, LLMs were thus the first models to demonstrate a more generalized ability to solve problems across a wide range of domains.

Since then, the development of LLMs more capable than the original models has continued and no end is in sight. The increase in these models' abilities can be attributed to several factors such as larger training datasets, higher context windows and even more advanced architectures.
It didn't take much longer until big tech companies began to advertise their models with claims that their models were capable of reasoning, and thus being more capable of solving problems in domains such as math, programming and logic.
The term LRM (large reasoning model) was even introduced to describe models with such capabilities.
While certainly a valuable selling point, the actual reasoning capabilities of LLMs have been a topic of debate among scholars ever since.
This study will not attempt to settle this debate in one way or another, but is instead concerned with the practical abilities LLMs showcase when confronted with rather complex problems.

To study their abilities when faced with reasoning tasks, researchers have come up with various benchmarks.
One of the most well known benachmarks is the FrontierMATH benchmarks

...

To summarize, while their abilities in reasoning tasks are by no means perfect, recent LLMs have proven to be more capable than ever before, and the trend is likely to continue in the foreseeable future.
This opens the door to confront LLMs with spatial reasoning tasks such as navigation, which will be the topic of the upcoming section.

\subsection{LLMs in Spatial Reasoning and Navigation}

As we have seen, LLMs abilities to perform complex tasks requiring the use of a process similar to multi step reasoning has increased without any signs of slowdown over the past five years.
Although we cannot infer from this fact alone that LLMs are capable of solving issues in spatial reasoning as well, these new abilities may hint at their use in the field of geographic information science (GIScience).
While the discipline has identified several promising applications for LLMs in the domain, not all the research has concerned itself with their abilities regarding navigation tasks.

Among several other applications, one paper published in 2025 by X et al. highlights the potential of LLMs to serve as natural language interfaces for GI systems (Geo-Information Systems):
In summary, LLMs could be used to allow users to interact with spatial data in new ways by using human language, thereby democratizing access to sophisticated GIS analysis.

If we want to apply this concept to navigation tasks however, we need to accept that LLMs have been shown to perform rather poorly in this area:
In a study conducted by Y et al. in 2025, several LLMs were tested on navigation tasks and their performance could be shown to be lacking.
While the exact reasons for LLMs shortcomings in this area are not part of this thesis, we theorize that a lack of qualitative geographic descripitions in the training data used to train these LLMs may be a significant factor.


While it was proposed that LLMs could serve as natural language interfaces, and their navigational capabilities were shown to be lacking, little work has been done to find ways to alleviate their shortcomings in this regard.
Other work focuses on the use of LLMs to create fully autonomous navigation agents using computer vision and robotics, which - although certainly impressive - is outside the scope of this thesis.
Despite the increase in reasoning capabilities of LLMs in other domains, which were presented in Chapter 1.2.1, using LLMs for navigation tasks seems to be a dead end as of today.
In the next section, we will present a proven technique to enhance LLM performance in various domains.

\subsection{Context Enrichment Techniques for LLMs}

After several studies presented convincing evidence concerning hallucinations and other shortcomings of LLMs, researchers began to look for possible techniques to reduce the occurence of such issues. 
With context enrichment techniques, one promising approach was quickly identified:
To account for blind spots in the datasets used to train the LLMs, researchers began to allow LLMs to access additional knowledge when presented with a user query.

\section{Research Gap}

We identify a gap in the current literature regarding the use of qualitative geographic context to enhance LLM navigation performance.
Without further research, any verdict on the viability of LLMs as natural language interfaces for navigation tasks will remain inconclusive. 
Previous research has adressed a) the general capabilities of LLMs in reasoning tasks, b) their potential use in GIScience applications as well as c) their shortcomings in navigation tasks.
This research is not sufficient to fully dismiss the idea of using LLMs for navigation, as few attempts have been made to improve their navigation performance.
Context enrichment has been shown to be a promising technique to reduce hallucinations in LLM responses, which often occur when they are confronted with spatial reasoning problems.
Further, the dipole calculus has been shown to be a valid qualitative representation of topological data.
Therefore, in ths study, we aim to quantify the impact of context enrichment using dipole relations on LLM navigation performance to provide first evidence on whether this technique could prove viable.
Additionaly, our testing framework may be used as a benchmark for future research utilizing different approaches in this area.
This way, we can thorougly study to which extent LLM navigation can be improved and reach a conclusion backed by empirical evidence.

\section{Research Hypothesis}

Drawing from the current state of research, we propose that context enrichment using qualitative geographic relations may have a positive effect on LLM navigation performance.
Context enrichment has been shown to make LLM responses less prone to hallucinations in other domains, while dipole relations provide a proven framework for qualitatively describing topological data.
Combined, these techniques should aid the LLM and allow it to provide more accurate answer to navigational tasks.
We thus formulate the following research hypothesis H1: 
\begin{quote} 
    \textit{The inclusion of qualitative geographic context in the form of natural language dipole relations significantly improves the navigation performance of large language models.}
\end{quote}
The alternative hypothesis can thus be formmulated as follows H0:
\begin{quote} 
    \textit{The inclusion of qualitative geographic context in the form of natural language dipole relations does not significantly improve the navigation performance of large language models.}
\end{quote}
In the following chapters, we will describe the methods used to test our hypothesis and present the results of our experiments.
The thesis will then conclude with a discussion of the findings and their meaning for future research in this area.