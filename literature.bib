@book{Hartley2004,
  title = {{Multiple View Geometry}}, 
  author = {Hartley, Richard and Zisserman, Andrew}, 
  edition = {3}, 
  isbn = {978-0521540513}, 
  year = {2004}
}
@book{Bishop2006, 
  title = {{Pattern Recognition And Machine Learning}}, 
  author = {Bishop, Christopher}, 
  edition = {1}, 
  isbn = {978-1-4939-3843-8}, 
  issn = {1613-9011}, 
  year = {2006}
}
@techreport{openai_unlocking_2025,
	title = {Unlocking {Economic} {Opportunity}: {A} {First} {Look} at {ChatGPT}-{Powered} {Productivity}},
	institution = {OpenAI},
	author = {{OpenAI}},
	month = jul,
	year = {2025},
	file = {OpenAI Productivity Note 1 [Jul 2025]:/Users/niklasdaute/Zotero/storage/IKB5SX93/OpenAI_Productivity-Note_Jul-2025.pdf:application/pdf},
}
@article{chatterji_how_2025,
	title = {How {People} {Use} {ChatGPT}},
	abstract = {Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10\% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages but even faster growth in non-work-related messages, which have grown from 53\% to more than 70\% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation topic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80\% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional search engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.},
	language = {en},
	author = {Chatterji, Aaron and Cunningham, Tom and Deming, David and Hitzig, Zoe and Ong, Christopher and Shan, Carl and Wadman, Kevin},
	month = sep,
	year = {2025},
	file = {PDF:/Users/niklasdaute/Zotero/storage/5GDJVHSG/Chatterji et al. - How People Use ChatGPT.pdf:application/pdf},
}

@misc{minaee_large_2025,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	urldate = {2025-12-09},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = mar,
	year = {2025},
	note = {arXiv:2402.06196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/4EYLB8NT/Minaee et al. - 2025 - Large Language Models A Survey.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/E7EJU6C4/2402.html:text/html},
}

@misc{xiong_autoregressive_2025,
	title = {Autoregressive {Models} in {Vision}: {A} {Survey}},
	shorttitle = {Autoregressive {Models} in {Vision}},
	url = {http://arxiv.org/abs/2411.05902},
	doi = {10.48550/arXiv.2411.05902},
	abstract = {Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the representation strategy. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multifaceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multimodal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.},
	urldate = {2025-12-09},
	publisher = {arXiv},
	author = {Xiong, Jing and Liu, Gongye and Huang, Lun and Wu, Chengyue and Wu, Taiqiang and Mu, Yao and Yao, Yuan and Shen, Hui and Wan, Zhongwei and Huang, Jinfa and Tao, Chaofan and Yan, Shen and Yao, Huaxiu and Kong, Lingpeng and Yang, Hongxia and Zhang, Mi and Sapiro, Guillermo and Luo, Jiebo and Luo, Ping and Wong, Ngai},
	month = may,
	year = {2025},
	note = {arXiv:2411.05902 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/2Y77JJCP/Xiong et al. - 2025 - Autoregressive Models in Vision A Survey.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/8HUDG853/2411.html:text/html},
}

@article{johnston_revisiting_2025,
	title = {Revisiting the problem of learning long-term dependencies in recurrent neural networks},
	volume = {183},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608024008165},
	doi = {10.1016/j.neunet.2024.106887},
	abstract = {Recurrent neural networks (RNNs) are an important class of models for learning sequential behavior. However, training RNNs to learn long-term dependencies is a tremendously difficult task, and this difficulty is widely attributed to the vanishing and exploding gradient (VEG) problem. Since it was first characterized 30 years ago, the belief that if VEG occurs during optimization then RNNs learn long-term dependencies poorly has become a central tenet in the RNN literature and has been steadily cited as motivation for a wide variety of research advancements. In this work, we revisit and interrogate this belief using a large factorial experiment where more than 40,000 RNNs were trained, and provide evidence contradicting this belief. Motivated by these findings, we re-examine the original discussion that analyzed latching behavior in RNNs by way of hyperbolic attractors, and ultimately demonstrate that these dynamics do not fully capture the learned characteristics of RNNs. Our findings suggest that these models are fully capable of learning dynamics that do not correspond to hyperbolic attractors, and that the choice of hyper-parameters, namely learning rate, has a substantial impact on the likelihood of whether an RNN will be able to learn long-term dependencies.},
	language = {en},
	urldate = {2025-12-09},
	journal = {Neural Networks},
	author = {Johnston, Liam and Patel, Vivak and Cui, Yumian and Balaprakash, Prasanna},
	month = mar,
	year = {2025},
	pages = {106887},
	file = {PDF:/Users/niklasdaute/Zotero/storage/53GD72DS/Johnston et al. - 2025 - Revisiting the problem of learning long-term dependencies in recurrent neural networks.pdf:application/pdf},
}
@article{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {PDF:/Users/niklasdaute/Zotero/storage/8X35X7AY/Vaswani et al. - Attention is All you Need.pdf:application/pdf},
}
@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample eﬃciency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
	language = {en},
	urldate = {2025-12-09},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/niklasdaute/Zotero/storage/KRY7UZLI/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf},
}

@article{huang_survey_2025,
	title = {A {Survey} on {Hallucination} in {Large} {Language} {Models}: {Principles}, {Taxonomy}, {Challenges}, and {Open} {Questions}},
	volume = {43},
	issn = {1046-8188, 1558-2868},
	shorttitle = {A {Survey} on {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.05232},
	doi = {10.1145/3703155},
	abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
	number = {2},
	urldate = {2025-12-10},
	journal = {ACM Transactions on Information Systems},
	author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
	month = mar,
	year = {2025},
	note = {arXiv:2311.05232 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {1--55},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/923PYBQ8/Huang et al. - 2025 - A Survey on Hallucination in Large Language Models Principles, Taxonomy, Challenges, and Open Quest.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/CFIVI84B/2311.html:text/html},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2025-12-10},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/niklasdaute/Zotero/storage/5YJFGTF3/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/Q2IZ2F78/2005.html:text/html},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	urldate = {2025-12-10},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/niklasdaute/Zotero/storage/79Y6SMNP/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/PDDUUKYE/2312.html:text/html},
}
@misc{edge_local_2025,
	title = {From {Local} to {Global}: {A} {Graph} {RAG} {Approach} to {Query}-{Focused} {Summarization}},
	shorttitle = {From {Local} to {Global}},
	url = {http://arxiv.org/abs/2404.16130},
	doi = {10.48550/arXiv.2404.16130},
	abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.},
	urldate = {2025-12-10},
	publisher = {arXiv},
	author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Metropolitansky, Dasha and Ness, Robert Osazuwa and Larson, Jonathan},
	month = feb,
	year = {2025},
	note = {arXiv:2404.16130 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/BL5AARIU/Edge et al. - 2025 - From Local to Global A Graph RAG Approach to Query-Focused Summarization.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/RXCJRG7R/2404.html:text/html},
}
@article{lwin_quantitative_2012,
	title = {Quantitative versus {Qualitative} {Geospatial} {Data} in {Spatial} {Modelling} and {Decision} {Making}},
	volume = {04},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2151-1950, 2151-1969},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/jgis.2012.43028},
	doi = {10.4236/jgis.2012.43028},
	abstract = {In general, geospatial data can be divided into two formats, raster and vector formats. A raster consists of a matrix of cells where each cell contains a value representing quantitative information, such as temperature, vegetation intensity, land use/cover, elevation, etc. A vector data consists of points, lines and polygons representing location or distance or area of landscape features in graphical forms. Many raster data are derived from remote sensing techniques using sophisticated sensors by quantitative approach and many vector data are generated from GIS processes by qualitative approach. Among them, land use/cover data is frequently used in many GIS analyses and spatial modeling processes. However, proper use of quantitative and qualitative geospatial data is important in spatial modeling and decision making. In this article, we discuss common geospatial data formats, their origins and proper use in spatial modelling and decision making processes.},
	language = {en},
	number = {03},
	urldate = {2025-12-10},
	journal = {Journal of Geographic Information System},
	author = {Lwin, Ko Ko and Murayama, Yuji and Mizutani, Chiaki},
	year = {2012},
	pages = {237--241},
	file = {PDF:/Users/niklasdaute/Zotero/storage/CGQYWVH6/Lwin et al. - 2012 - Quantitative versus Qualitative Geospatial Data in Spatial Modelling and Decision Making.pdf:application/pdf},
}
@article{moratz_condensed_2011,
	title = {A condensed semantics for qualitative spatial reasoning about oriented straight line segments},
	volume = {175},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370211000890},
	doi = {10.1016/j.artint.2011.07.004},
	abstract = {More than 15 years ago, a set of qualitative spatial relations between oriented straight line segments (dipoles) was suggested by Schlieder. However, it turned out to be diﬃcult to establish a sound constraint calculus based on these relations. In this paper, we present the results of a new investigation into dipole constraint calculi which uses algebraic methods to derive sound results on the composition of relations of dipole calculi. This new method, which we call condensed semantics, is based on an abstract symbolic model of a speciﬁc fragment of our domain. It is based on the fact that qualitative dipole relations are invariant under orientation preserving aﬃne transformations.},
	language = {en},
	number = {16-17},
	urldate = {2025-11-07},
	journal = {Artificial Intelligence},
	author = {Moratz, Reinhard and Lücke, Dominik and Mossakowski, Till},
	month = oct,
	year = {2011},
	pages = {2099--2127},
	file = {PDF:/Users/niklasdaute/Zotero/storage/VANFWVKF/Moratz et al. - 2011 - A condensed semantics for qualitative spatial reasoning about oriented straight line segments.pdf:application/pdf},
}
@article{haklay_openstreetmap_2008,
	title = {{OpenStreetMap}: {User}-{Generated} {Street} {Maps}},
	volume = {7},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1536-1268},
	shorttitle = {{OpenStreetMap}},
	url = {http://ieeexplore.ieee.org/document/4653466/},
	doi = {10.1109/MPRV.2008.80},
	language = {en},
	number = {4},
	urldate = {2025-12-10},
	journal = {IEEE Pervasive Computing},
	author = {Haklay, M. and Weber, P.},
	month = oct,
	year = {2008},
	pages = {12--18},
	file = {PDF:/Users/niklasdaute/Zotero/storage/KCM24W3C/Haklay and Weber - 2008 - OpenStreetMap User-Generated Street Maps.pdf:application/pdf},
}
@book{university_of_nottingham_gb_mapping_2017,
	title = {Mapping and the {Citizen} {Sensor}},
	isbn = {978-1-911529-16-3},
	url = {https://www.ubiquitypress.com/site/books/10.5334/bbf/},
	language = {en},
	urldate = {2025-12-10},
	publisher = {Ubiquity Press},
	author = {Antoniou, Vyron and {Geographic Directorate, PAPAGOU Camp, GR}},
	editor = {{University of Nottingham, GB} and Foody, Giles and See, Linda and {International Institute for Applied Systems Analysis (IIASA), AT} and Fritz, Steffen and {International Institute for Applied Systems Analysis (IIASA), AT} and Mooney, Peter and {Maynooth University, IE} and Olteanu-Raimond, Ana-Maria and {Paris-Est, LASTIG COGIT, FR} and Fonte, Cidália Costa and {University of Coimbra, PT}},
	month = sep,
	year = {2017},
	doi = {10.5334/bbf},
	file = {PDF:/Users/niklasdaute/Zotero/storage/8VT9KG5L/Antoniou and Geographic Directorate, PAPAGOU Camp, GR - 2017 - Mapping and the Citizen Sensor.pdf:application/pdf},
}

@article{sehra_assessment_2013,
	title = {Assessment of {OpenStreetMap} {Data} - {A} {Review}},
	volume = {76},
	issn = {09758887},
	url = {http://arxiv.org/abs/1309.6608},
	doi = {10.5120/13331-0888},
	abstract = {The meaning and purposes of web has been changing and evolving day by day. Web 2. 0 encouraged more contribution by the end users. This movement provided revolutionary methods of sharing and computing data by crowdsourcing such as OpenStreetmap, also called "the wikification of maps" by some researchers. When crowdsourcing collects huge data with help of general public with varying level of mapping experience, the focus of researcher should be on analysing the data rather than collecting it. Researchers have assessed the quality of OpenStreetMap data by comparing it with proprietary data or data of governmental map agencies. This study reviews the research work for assessment of Open- StreetMap Data and also discusses about the future directions.},
	number = {16},
	urldate = {2025-12-10},
	journal = {International Journal of Computer Applications},
	author = {Sehra, Sukhjit Singh and Singh, Jaiteg and Rai, Hardeep Singh},
	month = aug,
	year = {2013},
	note = {arXiv:1309.6608 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Databases, Computer Science - Social and Information Networks},
	pages = {17--20},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/ZYECN3M6/Sehra et al. - 2013 - Assessment of OpenStreetMap Data - A Review.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/CQAMJHTU/1309.html:text/html},
}

@article{neis_recent_2014,
	title = {Recent {Developments} and {Future} {Trends} in {Volunteered} {Geographic} {Information} {Research}: {The} {Case} of {OpenStreetMap}},
	volume = {6},
	issn = {1999-5903},
	shorttitle = {Recent {Developments} and {Future} {Trends} in {Volunteered} {Geographic} {Information} {Research}},
	url = {https://www.mdpi.com/1999-5903/6/1/76},
	doi = {10.3390/fi6010076},
	abstract = {User-generated content (UGC) platforms on the Internet have experienced a steep increase in data contributions in recent years. The ubiquitous usage of location-enabled devices, such as smartphones, allows contributors to share their geographic information on a number of selected online portals. The collected information is oftentimes referred to as volunteered geographic information (VGI). One of the most utilized, analyzed and cited VGI-platforms, with an increasing popularity over the past few years, is OpenStreetMap (OSM), whose main goal it is to create a freely available geographic database of the world. This paper presents a comprehensive overview of the latest developments in VGI research, focusing on its collaboratively collected geodata and corresponding contributor patterns. Additionally, trends in the realm of OSM research are discussed, highlighting which aspects need to be investigated more closely in the near future.},
	language = {en},
	number = {1},
	urldate = {2025-12-10},
	journal = {Future Internet},
	author = {Neis, Pascal and Zielstra, Dennis},
	month = jan,
	year = {2014},
	pages = {76--106},
	file = {PDF:/Users/niklasdaute/Zotero/storage/LMHH5LBC/Neis and Zielstra - 2014 - Recent Developments and Future Trends in Volunteered Geographic Information Research The Case of Op.pdf:application/pdf},
}
@inproceedings{luxen_real-time_2011,
	address = {Chicago Illinois},
	title = {Real-time routing with {OpenStreetMap} data},
	isbn = {978-1-4503-1031-4},
	url = {https://dl.acm.org/doi/10.1145/2093973.2094062},
	doi = {10.1145/2093973.2094062},
	language = {en},
	urldate = {2025-12-10},
	booktitle = {Proceedings of the 19th {ACM} {SIGSPATIAL} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {ACM},
	author = {Luxen, Dennis and Vetter, Christian},
	month = nov,
	year = {2011},
	pages = {513--516},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/R3KK3E6S/Luxen and Vetter - 2011 - Real-time routing with OpenStreetMap data.pdf:application/pdf},
}
@article{boeing_osmnx_2017,
	title = {{OSMnx}: {New} methods for acquiring, constructing, analyzing, and visualizing complex street networks},
	volume = {65},
	issn = {01989715},
	shorttitle = {{OSMnx}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0198971516303970},
	doi = {10.1016/j.compenvurbsys.2017.05.004},
	language = {en},
	urldate = {2025-12-10},
	journal = {Computers, Environment and Urban Systems},
	author = {Boeing, Geoff},
	month = sep,
	year = {2017},
	pages = {126--139},
	file = {PDF:/Users/niklasdaute/Zotero/storage/2Y6XSWHB/Boeing - 2017 - OSMnx New methods for acquiring, constructing, analyzing, and visualizing complex street networks.pdf:application/pdf},
}

@misc{noauthor_google_2025,
	title = {Google {Maps}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Google_Maps&oldid=1325235575},
	abstract = {Google Maps is a web mapping platform and consumer application developed by Google. It offers satellite imagery, aerial photography, street maps, 360° interactive panoramic views of streets (Street View), real-time traffic conditions, and route planning for traveling by foot, car, bike, air (in beta) and public transportation. As of 2020, Google Maps was being used by over one billion people every month around the world.
Google Maps began as a C++ desktop program developed by brothers Lars and Jens Rasmussen, Stephen Ma and Noel Gordon in Australia at Where 2 Technologies. In October 2004, the company was acquired by Google, which converted it into a web application. After additional acquisitions of a geospatial data visualization company and a real-time traffic analyzer, Google Maps was launched in February 2005. The service's front end utilizes JavaScript, XML, and Ajax. Google Maps offers an API that allows maps to be embedded on third-party websites, and offers a locator for businesses and other organizations in numerous countries around the world. Google Map Maker allowed users to collaboratively expand and update the service's mapping worldwide but was discontinued from March 2017. However, crowdsourced contributions to Google Maps were not discontinued as the company announced those features would be transferred to the Google Local Guides program, although users that are not Local Guides can still contribute.

Google Maps' satellite view is a "top-down" or bird's-eye view; most of the high-resolution imagery of cities is aerial photography taken from aircraft flying at 800 to 1,500 feet (240 to 460 m), while most other imagery is from satellites. Much of the available satellite imagery is no more than three years old and is updated on a regular basis, according to a 2011 report. Google Maps previously used a variant of the Mercator projection, and therefore could not accurately show areas around the poles. In August 2018, the desktop version of Google Maps was updated to show a 3D globe. It is still possible to switch back to the 2D map in the settings.
Google Maps for mobile devices was first released in 2006; the latest versions feature GPS turn-by-turn navigation along with dedicated parking assistance features. By 2013, it was found to be the world's most popular smartphone app, with over 54\% of global smartphone owners using it. In 2017, the app was reported to have two billion users on Android, along with several other Google services including YouTube, Chrome, Gmail, Search, and Google Play.},
	language = {en},
	urldate = {2025-12-11},
	journal = {Wikipedia},
	month = dec,
	year = {2025},
	note = {Page Version ID: 1325235575},
	file = {Snapshot:/Users/niklasdaute/Zotero/storage/J3BVALHB/index.html:text/html},
}

@misc{noauthor_look_2020,
	title = {A look back at 15 years of mapping the world},
	url = {https://blog.google/products/maps/look-back-15-years-mapping-world/},
	abstract = {As Google Maps’ birthday approaches, it’s the perfect time to remember how far we’ve come.},
	language = {en-us},
	urldate = {2025-12-11},
	journal = {Google},
	month = feb,
	year = {2020},
	file = {Snapshot:/Users/niklasdaute/Zotero/storage/FP4V38PY/look-back-15-years-mapping-world.html:text/html},
}
@misc{noauthor_popularity_nodate,
	title = {The {Popularity} of {Google} {Maps}: {Trends} in {Navigation} {Apps} in 2018},
	url = {https://themanifest.com/app-development/trends-navigation-apps},
	urldate = {2025-12-11},
	file = {The Popularity of Google Maps\: Trends in Navigation Apps in 2018:/Users/niklasdaute/Zotero/storage/PMAFU9I7/trends-navigation-apps.html:text/html},
}

@misc{noauthor_google_2019,
	title = {Google {Maps} 101: how we map the world},
	shorttitle = {Google {Maps} 101},
	url = {https://blog.google/products/maps/google-maps-101-how-we-map-world/},
	abstract = {A closer look at how we build Google Maps.},
	language = {en-us},
	urldate = {2025-12-11},
	journal = {Google},
	month = jul,
	year = {2019},
	file = {Snapshot:/Users/niklasdaute/Zotero/storage/J35XXKHF/google-maps-101-how-we-map-world.html:text/html},
}

@misc{noauthor_apple_nodate,
	title = {Apple {Previews} {iOS} 6 {With} {All} {New} {Maps}, {Siri} {Features}, {Facebook} {Integration}, {Shared} {Photo} {Streams} \& {New} {Passbook} {App}},
	url = {https://www.apple.com/newsroom/2012/06/11Apple-Previews-iOS-6-With-All-New-Maps-Siri-Features-Facebook-Integration-Shared-Photo-Streams-New-Passbook-App/},
	language = {en-US},
	urldate = {2025-12-11},
	journal = {Apple Newsroom},
}

@article{steyerberg_prediction_2016,
	title = {Prediction models need appropriate internal, internal-external, and external validation},
	volume = {69},
	issn = {0895-4356},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC5578404/},
	doi = {10.1016/j.jclinepi.2015.04.005},
	urldate = {2025-12-11},
	journal = {Journal of clinical epidemiology},
	author = {Steyerberg, Ewout W. and Harrell, Frank E.},
	month = jan,
	year = {2016},
	pmid = {25981519},
	pmcid = {PMC5578404},
	pages = {245--247},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/PBS3EK3T/Steyerberg and Harrell - 2016 - Prediction models need appropriate internal, internal-external, and external validation.pdf:application/pdf},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	doi = {10.48550/arXiv.1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv:1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/niklasdaute/Zotero/storage/8FRLS7SU/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/ER3PUJUX/1804.html:text/html},
}

@misc{wang_superglue_2020,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	doi = {10.48550/arXiv.1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv:1905.00537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/niklasdaute/Zotero/storage/KSPHPCER/Wang et al. - 2020 - SuperGLUE A Stickier Benchmark for General-Purpose Language Understanding Systems.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/45ID56X5/1905.html:text/html},
}
@misc{nangia_human_2019,
	title = {Human vs. {Muppet}: {A} {Conservative} {Estimate} of {Human} {Performance} on the {GLUE} {Benchmark}},
	shorttitle = {Human vs. {Muppet}},
	url = {http://arxiv.org/abs/1905.10425},
	doi = {10.48550/arXiv.1905.10425},
	abstract = {The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Nangia, Nikita and Bowman, Samuel R.},
	month = jun,
	year = {2019},
	note = {arXiv:1905.10425 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/niklasdaute/Zotero/storage/7U3IL69S/Nangia and Bowman - 2019 - Human vs. Muppet A Conservative Estimate of Human Performance on the GLUE Benchmark.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/85JVEMGC/1905.html:text/html},
}

@misc{srivastava_beyond_2023,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2025-12-12},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and Melo, Gerard de and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and Hoeve, Maartje ter and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan A. and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2023},
	note = {arXiv:2206.04615 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/niklasdaute/Zotero/storage/AKVESVCP/Srivastava et al. - 2023 - Beyond the Imitation Game Quantifying and extrapolating the capabilities of language models.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/VB286JW6/2206.html:text/html},
}





