@book{Hartley2004,
  title = {{Multiple View Geometry}}, 
  author = {Hartley, Richard and Zisserman, Andrew}, 
  edition = {3}, 
  isbn = {978-0521540513}, 
  year = {2004}
}
@book{Bishop2006, 
  title = {{Pattern Recognition And Machine Learning}}, 
  author = {Bishop, Christopher}, 
  edition = {1}, 
  isbn = {978-1-4939-3843-8}, 
  issn = {1613-9011}, 
  year = {2006}
}
@techreport{openai_unlocking_2025,
	title = {Unlocking {Economic} {Opportunity}: {A} {First} {Look} at {ChatGPT}-{Powered} {Productivity}},
	institution = {OpenAI},
	author = {{OpenAI}},
	month = jul,
	year = {2025},
	file = {OpenAI Productivity Note 1 [Jul 2025]:/Users/niklasdaute/Zotero/storage/IKB5SX93/OpenAI_Productivity-Note_Jul-2025.pdf:application/pdf},
}
@article{chatterji_how_2025,
	title = {How {People} {Use} {ChatGPT}},
	abstract = {Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10\% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages but even faster growth in non-work-related messages, which have grown from 53\% to more than 70\% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation topic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80\% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional search engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.},
	language = {en},
	author = {Chatterji, Aaron and Cunningham, Tom and Deming, David and Hitzig, Zoe and Ong, Christopher and Shan, Carl and Wadman, Kevin},
	month = sep,
	year = {2025},
	file = {PDF:/Users/niklasdaute/Zotero/storage/5GDJVHSG/Chatterji et al. - How People Use ChatGPT.pdf:application/pdf},
}

@misc{minaee_large_2025,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	urldate = {2025-12-09},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = mar,
	year = {2025},
	note = {arXiv:2402.06196 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/4EYLB8NT/Minaee et al. - 2025 - Large Language Models A Survey.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/E7EJU6C4/2402.html:text/html},
}

@misc{xiong_autoregressive_2025,
	title = {Autoregressive {Models} in {Vision}: {A} {Survey}},
	shorttitle = {Autoregressive {Models} in {Vision}},
	url = {http://arxiv.org/abs/2411.05902},
	doi = {10.48550/arXiv.2411.05902},
	abstract = {Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the representation strategy. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multifaceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multimodal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.},
	urldate = {2025-12-09},
	publisher = {arXiv},
	author = {Xiong, Jing and Liu, Gongye and Huang, Lun and Wu, Chengyue and Wu, Taiqiang and Mu, Yao and Yao, Yuan and Shen, Hui and Wan, Zhongwei and Huang, Jinfa and Tao, Chaofan and Yan, Shen and Yao, Huaxiu and Kong, Lingpeng and Yang, Hongxia and Zhang, Mi and Sapiro, Guillermo and Luo, Jiebo and Luo, Ping and Wong, Ngai},
	month = may,
	year = {2025},
	note = {arXiv:2411.05902 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/niklasdaute/Zotero/storage/2Y77JJCP/Xiong et al. - 2025 - Autoregressive Models in Vision A Survey.pdf:application/pdf;Snapshot:/Users/niklasdaute/Zotero/storage/8HUDG853/2411.html:text/html},
}


